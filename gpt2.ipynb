{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "758ff223-c147-4954-8975-2cdbfcf66fa9",
   "metadata": {},
   "source": [
    "# Text Generation with GPT2 and Hugging Face\n",
    "\n",
    "See [this overview](https://huggingface.co/course/chapter1/1) for an introduction to Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c09c68-10d1-48bc-b753-ce387c25edb8",
   "metadata": {},
   "source": [
    "## Load Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb43c842-79b3-4cd1-9ecb-71ed5ed4a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# May need to run the following command to install Hugging Face\n",
    "# !pip install transformers\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, GPT2LMHeadModel,\n",
    "    StoppingCriteriaList, MaxLengthCriteria\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc846781-0012-4090-b33f-4ebdce5e84cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5184c69b-910a-4e22-a31d-1a5b52e7c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of next tokens and probabilities to output\n",
    "num_tokens = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadcf9e9-d0fe-4f97-9bd6-7f5745a0d666",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "719ba06e-fc20-492f-b2bd-b69b87393a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify our prompt\n",
    "prompt = 'AI will support banks by'\n",
    "\n",
    "# Encode the input\n",
    "inputs = tokenizer(prompt, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6093fbe1-2ca5-43c7-a379-ceb3b9d31c9f",
   "metadata": {},
   "source": [
    "### First Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28f5a5d0-f17b-4953-9eef-55f5d3cb98a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "outputs = model(**inputs, labels=inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "199bcb4c-9f46-438f-a48c-85f24b605cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logits and convert to probabilities\n",
    "logits = outputs.logits\n",
    "probs = logits.softmax(axis=-1)[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "808d3a34-7399-4eb2-9eeb-e75ecfd92171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' providing', ' offering', ' making']\n",
      "tensor([0.1313, 0.0586, 0.0254], grad_fn=<TopkBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Get maximum token probabilities\n",
    "max_probs = torch.topk(probs.flatten(), num_tokens)\n",
    "tokens = [tokenizer.decode(x) for x in max_probs.indices]\n",
    "\n",
    "print(tokens)\n",
    "print(max_probs.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8257262-c281-4306-9a5e-70cdd06357ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI will support banks by providing\n"
     ]
    }
   ],
   "source": [
    "# Append most likely token to prompt\n",
    "prompt = prompt + tokens[0]\n",
    "\n",
    "# Output result\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ce16c2-372b-4f7d-a4df-35e22499fb91",
   "metadata": {},
   "source": [
    "### Additional Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fd8bc48-3f0c-4063-b3a1-cf5ba69934e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities 0: tensor([0.0935, 0.0558, 0.0315], grad_fn=<TopkBackward0>)\n",
      "Tokens 0: [' a', ' them', ' loans']\n",
      "Prompt 0: AI will support banks by providing a\n",
      "\n",
      "Probabilities 1: tensor([0.0221, 0.0184, 0.0183], grad_fn=<TopkBackward0>)\n",
      "Tokens 1: [' range', ' secure', ' \"']\n",
      "Prompt 1: AI will support banks by providing a range\n",
      "\n",
      "Probabilities 2: tensor([9.9720e-01, 4.8669e-04, 2.6866e-04], grad_fn=<TopkBackward0>)\n",
      "Tokens 2: [' of', ' and', ' to']\n",
      "Prompt 2: AI will support banks by providing a range of\n",
      "\n",
      "Probabilities 3: tensor([0.0877, 0.0449, 0.0210], grad_fn=<TopkBackward0>)\n",
      "Tokens 3: [' services', ' financial', ' loans']\n",
      "Prompt 3: AI will support banks by providing a range of services\n",
      "\n",
      "Probabilities 4: tensor([0.2312, 0.1404, 0.1302], grad_fn=<TopkBackward0>)\n",
      "Tokens 4: [' to', ' including', ',']\n",
      "Prompt 4: AI will support banks by providing a range of services to\n",
      "\n",
      "Probabilities 5: tensor([0.0538, 0.0518, 0.0472], grad_fn=<TopkBackward0>)\n",
      "Tokens 5: [' help', ' the', ' ensure']\n",
      "Prompt 5: AI will support banks by providing a range of services to help\n",
      "\n",
      "Probabilities 6: tensor([0.2145, 0.0594, 0.0308], grad_fn=<TopkBackward0>)\n",
      "Tokens 6: [' them', ' customers', ' ensure']\n",
      "Prompt 6: AI will support banks by providing a range of services to help them\n",
      "\n",
      "Probabilities 7: tensor([0.0782, 0.0691, 0.0443], grad_fn=<TopkBackward0>)\n",
      "Tokens 7: [' meet', ' manage', ' achieve']\n",
      "Prompt 7: AI will support banks by providing a range of services to help them meet\n",
      "\n",
      "Probabilities 8: tensor([0.4160, 0.1569, 0.0307], grad_fn=<TopkBackward0>)\n",
      "Tokens 8: [' their', ' the', ' its']\n",
      "Prompt 8: AI will support banks by providing a range of services to help them meet their\n",
      "\n",
      "Probabilities 9: tensor([0.1052, 0.0793, 0.0733], grad_fn=<TopkBackward0>)\n",
      "Tokens 9: [' financial', ' customers', ' obligations']\n",
      "Prompt 9: AI will support banks by providing a range of services to help them meet their financial\n",
      "\n",
      "Probabilities 10: tensor([0.7678, 0.1278, 0.0225], grad_fn=<TopkBackward0>)\n",
      "Tokens 10: [' needs', ' obligations', ' need']\n",
      "Prompt 10: AI will support banks by providing a range of services to help them meet their financial needs\n",
      "\n",
      "Probabilities 11: tensor([0.5435, 0.1625, 0.0858], grad_fn=<TopkBackward0>)\n",
      "Tokens 11: ['.', ',', ' and']\n",
      "Prompt 11: AI will support banks by providing a range of services to help them meet their financial needs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_tokens = 12\n",
    "\n",
    "for i in range(output_tokens):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "    logits = outputs.logits\n",
    "    probs = logits.softmax(axis=-1)[:, -1, :]\n",
    "\n",
    "    max_probs = torch.topk(probs.flatten(), num_tokens)\n",
    "    tokens = [tokenizer.decode(x) for x in max_probs.indices]\n",
    "\n",
    "    print(f'Probabilities {i}: {max_probs.values}')\n",
    "    print(f'Tokens {i}: {tokens}')\n",
    "\n",
    "    prompt = prompt + tokens[0]\n",
    "    print(f'Prompt {i}: {prompt}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
